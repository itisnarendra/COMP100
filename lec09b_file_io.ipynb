{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File Input and Output\n",
    "\n",
    "1. [File modes](#modes)\n",
    "1. [File object attributes](#attributes)\n",
    "1. [Basic file writing](#writing)\n",
    "    1. Writing a string to a file\n",
    "    1. Writing multiple lines to a file \n",
    "1. [ Basic file reading](#reading)\n",
    "    1. Reading the entire file at once\n",
    "    1. Reading line by line\n",
    "    1. Reading all lines into a list\n",
    "1. [Some simple files operations](#)\n",
    "    1. Using different encodings\n",
    "    1. Working with csv\n",
    "    1. Working with json\n",
    "1. [Some advanced file operations](#)\n",
    "    1. Reading and Processing Large Files Efficiently\n",
    "    1. CSV File Processing with Custom Delimiters\n",
    "    1. JSON File Operations with Error Handling\n",
    "    1. Binary File Operations with Struct Packing\n",
    "    1. Memory-Mapped File for Large Binary Files\n",
    "    1. File Locking for Concurrent Access\n",
    "    1. Temporary File Operations\n",
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When a file is open, you get a file object which you can use to perform write and or write on the underlining file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"modes\"></a>Modes for opening a \n",
    "File mode determines what operations can be done on the file object\n",
    "\n",
    "Character\tMeaning\n",
    "-   `r`\topen for reading (default)\n",
    "-   `t`\ttext mode (default)\n",
    "-   `w`\topen for writing, truncating the file first\n",
    "-   `x`\tcreate a new file and open it for writing.   \n",
    "    This will fail if the file already exist.\n",
    "-   `a`\topen for writing, appending to the end of the file if it exists\n",
    "-   `b`\tbinary mode\n",
    "-   `+`\topen a disk file for updating (reading and writing)\n",
    "\n",
    "Default mode is set for reading text files using encoding `utf-8`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"attributes\"></a>Exploring some attribute of the file object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('Data/person.txt', 'wt')\n",
    "print(f'    name: {f.name}')\n",
    "print(f'    mode: {f.mode}')\n",
    "print(f'encoding: {f.encoding}')\n",
    "print(f'  closed: {f.closed}')\n",
    "print(f'  fileno: {f.fileno()}')\n",
    "print(f'    tell: {f.tell()}')\n",
    "\n",
    "f.close()\n",
    "print(f'  closed: {f.closed}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='writing'></a> Basic file writing  \n",
    "1. Writing a string to a file using `write()`\n",
    "1. Writing multiple lines to a file using `writelines()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('Data/person.txt', 'wt')       #if the file exist, it will be overwritten\n",
    "f.write('Created on jan-18-2025 by Narendra\\n')\n",
    "f.write('12 times table\\n')\n",
    "f.write('--------------\\n')\n",
    "for x in range(1, 13):\n",
    "    f.write(f'{x} x 12 = {x * 12}\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the file is open in append mode, then write puts text at the end of the existing file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('Data/person.txt', 'at')\n",
    "f.write('this should be at the bottom of the file')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ['Created on jan-18-2025 by Narendra\\n',\n",
    "        '12 times table\\n', \n",
    "        '--------------\\n']\n",
    "\n",
    "table = [f'{x:>3} x {12} = {x * 12} \\n' for x in range(1, 13) ]\n",
    "\n",
    "data.extend(table) \n",
    "# notice that all the items in the list are string that ends with '\\n'\n",
    "# so we can use writelines() to write all the lines at once\n",
    "f = open('Data/table.txt', 'wt')\n",
    "f.writelines(data)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = open('person.dat', 'wb')\n",
    "# f.write(b'Created on jan-18-2025 by Narendra\\n')\n",
    "# f.write(b'12 times table\\n')\n",
    "# f.write(b'--------------\\n')\n",
    "# for x in range(1, 13):\n",
    "#     line = f'{x} x 12 = {x * 12}'\n",
    "#     f.write(bytearray(line, encoding ='utf-8'))\n",
    "#     f.write(b'\\n')\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='writing'></a>Basic file reading  \n",
    "1. `read(size=-1)` reads the entire file and returns it as a single string   \n",
    "       read() also takes an optional argument, an int representing the   \n",
    "       number of number of characters to read.\n",
    "1. `readline(size=-1)` reads one line\n",
    "1. `readlines(hint=-1)` reads the entire file and returns it as a list of  \n",
    "       strings. Each line is an item in the list. This also reads  \n",
    "       and captures the newline character. \n",
    "       Like read() it can take an int representing the number of  \n",
    "       lines to read.   \n",
    "1. `tell()` give the current stream position in the file\n",
    "1. `seek(cookie=-1)` moves the stream position "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('Data/person.txt')\n",
    "print(f'      first read: {f.read(10)}')            # read first 10 characters\n",
    "print(f'current position: {f.tell()}')              # current position in the file\n",
    "print(f'     the balance: {f.read()}')\n",
    "print('\\n\\nresetting to the start of the file')\n",
    "f.seek(0)                                           # move back to the start of the file\n",
    "print(f.read())                                       # read the entire file\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('Data/person.txt')\n",
    "print(f'{f.readline(20)}')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('Data/person.txt')\n",
    "print(f'{f.readlines()}')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below the the perferred way of reading a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('Data/person.txt')\n",
    "for line in f:\n",
    "    print(line.strip())  # strip() removes the trailing newline character\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with Binary Files\n",
    "Binary files are more compact and can be a slight deterrent to prying eyes.   \n",
    "Only binary data can be written to a file.   \n",
    "Reading will give binary data.   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing binary data\n",
    "data = bytes([0, 1, 2, 3, 4, 5])\n",
    "with open('Data/binary.bin', 'wb') as file:\n",
    "    # file.write('this is some plain text\\n')           #this does not write correctly\n",
    "    file.write(b'this is some binary text\\n')\n",
    "    file.write('this is a binary file\\n'.encode('utf-8'))\n",
    "    file.write(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading binary data\n",
    "\n",
    "with open('Data/binary.bin', 'rb') as file:\n",
    "    binary_data = file.read()\n",
    "    print(binary_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(b'hello world')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('person.txt') as f:\n",
    "    print('file contents:')\n",
    "    print(f'{f.readline()}')\n",
    "    print(f'{f.read()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Different Encodings\n",
    "In Python, encoding in file writing refers to the way text characters are converted into bytes before being stored in a file.\n",
    "\n",
    "When you write to a file in text mode, Python takes your string (which internally uses Unicode) and encodes it into a sequence of bytes according to the specified character encoding scheme — like UTF-8, UTF-16, ASCII, etc.\n",
    "\n",
    "#### Why encoding matters\n",
    "Computers store files as bytes, but human-readable text is made of characters.    \n",
    "Different encodings map characters to bytes differently.    \n",
    "If you choose the wrong encoding when writing, the file might not display correctly when read later.\n",
    "\n",
    "#### Common encodings\n",
    "**UTF-8** (default in Python 3.9+ on most systems) universal, supports all Unicode characters.    \n",
    "**ASCII** very limited (only English letters, numbers, and basic symbols).    \n",
    "**UTF-16 / UTF-32** wider byte representation, often used for compatibility. \n",
    "\n",
    "#### Suggestion:\n",
    "When you write a file with a specific encoding, read it later with the same encoding.   \n",
    "UTF-8 is almost always the safest choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing with specific encoding\n",
    "with open('Data/utf8_example.txt', 'w', encoding='utf-8') as file:\n",
    "    file.write(\"Some text with special characters: ñ, é, ü\\n\")\n",
    "\n",
    "# Reading with specific encoding\n",
    "with open('Data/utf8_example.txt', 'r', encoding='utf-8') as file:\n",
    "    print(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#another example\n",
    "with open('Data/utf8_example.txt', 'w', encoding='utf-8') as file:\n",
    "    file.write(\"Hello, world! — こんにちは — Привет\")\n",
    "\n",
    "# Reading with specific encoding\n",
    "with open('Data/utf8_example.txt', 'r', encoding='utf-8') as file:\n",
    "    print(file.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. working with CSV Files\n",
    "CSV files, short for Comma-Separated Values files, are a simple and widely-used format for storing tabular data—like what you’d see in a spreadsheet.\n",
    "\n",
    "What is a CSV file?\n",
    "A CSV file is a plain text file where:\n",
    "\n",
    "Each line represents a row of data.\n",
    "Each value in the row is separated by a comma (or sometimes another delimiter like a semicolon or tab).\n",
    "\n",
    "\n",
    "#### Why are CSV files important?\n",
    "Here are some key reasons:\n",
    "\n",
    "1.  Simplicity & Universality   \n",
    "CSV files are easy to create, read and edit. Almost every programming language and data tool (like Excel, Google Sheets, databases and programming languages like Python, R, etc.) supports them.\n",
    "\n",
    "1.  Lightweight Format    \n",
    "Since they’re plain text, CSV files are small in size and quick to load, process or transfer.\n",
    "\n",
    "1.  Easy Data Exchange   \n",
    "They’re ideal for sharing data between different systems or platforms—especially when exporting or importing data from databases, spreadsheets, or web applications.\n",
    "\n",
    "1.  Human-Readable    \n",
    "You can open a CSV file in any text editor and understand the data without needing special software.\n",
    "\n",
    "1.  Automation-Friendly    \n",
    "CSVs are often used in data pipelines, machine learning workflows, and automated reporting systems because they’re easy to parse and manipulate programmatically.\n",
    "\n",
    "1.  Integration with analytics tools   \n",
    "Many data analysis tools (Pandas, R, Tableau, Power BI) directly read CSVs.\n",
    "\n",
    "#### Limitations of CSV Files\n",
    "-   No support for formatting (colors, formulas, multiple sheets like Excel)\n",
    "-   Can’t handle complex data relationships (like a database)\n",
    "-   Risk of errors if data contains commas or line breaks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing to a CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "data = [\n",
    "    ['Name', 'Age', 'City'],\n",
    "    ['Alice', 30, 'Toronto'],\n",
    "    ['Bob', 25, 'Vancouver']\n",
    "]\n",
    "\n",
    "with open('Data/people.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing to a CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Data/people.csv', 'r', newline='') as file:\n",
    "    reader = csv.reader(file)\n",
    "    for row in reader:\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Working with json files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing to a JSON File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "data = {\n",
    "    'name': 'Alice',\n",
    "    'age': 30,\n",
    "    'city': 'Toronto'\n",
    "}\n",
    "\n",
    "with open('Data/data.json', 'w') as file:\n",
    "    json.dump(data, file, indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading a JSON files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Data/data.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "    print(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced File I/O Examples in Python\n",
    "Here are some more complex file operations that demonstrate practical scenarios:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Reading and Processing Large Files Efficiently\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_large_file(filename):\n",
    "    \"\"\"Process a large file line by line without loading it all into memory\"\"\"\n",
    "    with open(filename, 'r') as file:\n",
    "        for i, line in enumerate(file, 1):\n",
    "            # Process each line (example: count words)\n",
    "            words = line.strip().split()\n",
    "            print(f\"Line {i}: {len(words)} words\")\n",
    "            # You could yield lines for streaming processing\n",
    "            # yield process_line(line)\n",
    "\n",
    "# Usage:\n",
    "# process_large_file('big_data.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. CSV File Processing with Custom Delimiters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def process_csv(input_file, output_file):\n",
    "    with open(input_file, 'r') as infile, open(output_file, 'w', newline='') as outfile:\n",
    "        reader = csv.reader(infile, delimiter='|')\n",
    "        writer = csv.writer(outfile, delimiter='\\t')\n",
    "        \n",
    "        header = next(reader)  # Skip header\n",
    "        writer.writerow([h.upper() for h in header])\n",
    "        \n",
    "        for row in reader:\n",
    "            # Process each row (example: convert second column to uppercase)\n",
    "            if len(row) >= 2:\n",
    "                row[1] = row[1].upper()\n",
    "            writer.writerow(row)\n",
    "\n",
    "# Usage:\n",
    "# process_csv('input.csv', 'output.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. JSON File Operations with Error Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "def log_to_json(filename, message, level=\"INFO\"):\n",
    "    try:\n",
    "        # Try to read existing logs\n",
    "        try:\n",
    "            with open(filename, 'r') as file:\n",
    "                logs = json.load(file)\n",
    "        except (FileNotFoundError, json.JSONDecodeError):\n",
    "            logs = []\n",
    "        \n",
    "        # Add new log entry\n",
    "        logs.append({\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"level\": level,\n",
    "            \"message\": message\n",
    "        })\n",
    "        \n",
    "        # Write back to file\n",
    "        with open(filename, 'w') as file:\n",
    "            json.dump(logs, file, indent=2)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to write log: {e}\")\n",
    "\n",
    "# Usage:\n",
    "# log_to_json('app_logs.json', 'Application started')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Binary File Operations with Struct Packing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct\n",
    "\n",
    "def write_binary_data(filename, data):\n",
    "    \"\"\"Write a list of tuples as binary data\"\"\"\n",
    "    with open(filename, 'wb') as file:\n",
    "        for item in data:\n",
    "            # Pack as 1 integer, 1 float, and 10-character string\n",
    "            packed = struct.pack('if10s', item[0], item[1], item[2].encode('ascii'))\n",
    "            file.write(packed)\n",
    "\n",
    "def read_binary_data(filename):\n",
    "    \"\"\"Read binary data back into Python objects\"\"\"\n",
    "    results = []\n",
    "    with open(filename, 'rb') as file:\n",
    "        while True:\n",
    "            # Read 18 bytes at a time (4 for int, 4 for float, 10 for string)\n",
    "            chunk = file.read(18)\n",
    "            if not chunk:\n",
    "                break\n",
    "            # Unpack the binary data\n",
    "            unpacked = struct.unpack('if10s', chunk)\n",
    "            results.append((\n",
    "                unpacked[0],\n",
    "                unpacked[1],\n",
    "                unpacked[2].decode('ascii').strip('\\x00')\n",
    "            ))\n",
    "    return results\n",
    "\n",
    "# Usage:\n",
    "# data = [(1, 3.14, 'circle'), (2, 2.71, 'square')]\n",
    "# write_binary_data('shapes.bin', data)\n",
    "# print(read_binary_data('shapes.bin'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Memory-Mapped File for Large Binary Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mmap\n",
    "\n",
    "def search_in_large_file(filename, search_term):\n",
    "    \"\"\"Search for a string in a very large file using memory mapping\"\"\"\n",
    "    with open(filename, 'rb') as file:\n",
    "        # Memory-map the file\n",
    "        with mmap.mmap(file.fileno(), 0, access=mmap.ACCESS_READ) as mm:\n",
    "            # Search for the term (convert to bytes if needed)\n",
    "            if isinstance(search_term, str):\n",
    "                search_term = search_term.encode('utf-8')\n",
    "            \n",
    "            index = mm.find(search_term)\n",
    "            if index != -1:\n",
    "                # Show context around the found term\n",
    "                start = max(0, index - 20)\n",
    "                end = min(len(mm), index + len(search_term) + 20)\n",
    "                print(f\"Found at position {index}: {mm[start:end].decode('utf-8', errors='replace')}\")\n",
    "\n",
    "# Usage:\n",
    "# search_in_large_file('large_file.bin', 'important data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. File Locking for Concurrent Access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fcntl\n",
    "import time\n",
    "\n",
    "def safe_write(filename, data):\n",
    "    \"\"\"Write to a file with exclusive lock to prevent concurrent access issues\"\"\"\n",
    "    with open(filename, 'a') as file:\n",
    "        try:\n",
    "            # Try to acquire exclusive lock (non-blocking)\n",
    "            fcntl.flock(file, fcntl.LOCK_EX | fcntl.LOCK_NB)\n",
    "            \n",
    "            # Critical section\n",
    "            file.write(f\"{time.time()}: {data}\\n\")\n",
    "            \n",
    "            # Release lock (happens automatically when file closes)\n",
    "        except BlockingIOError:\n",
    "            print(\"File is locked by another process. Retrying...\")\n",
    "            time.sleep(1)\n",
    "            safe_write(filename, data)  # Retry\n",
    "\n",
    "# Usage:\n",
    "# safe_write('concurrent.log', 'Processed item 42')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Temporary File Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import os\n",
    "\n",
    "def process_with_temp_file(data):\n",
    "    \"\"\"Create and use a temporary file that auto-deletes when done\"\"\"\n",
    "    with tempfile.NamedTemporaryFile(mode='w+', delete=True) as temp_file:\n",
    "        # Write to the temp file\n",
    "        temp_file.write(data)\n",
    "        temp_file.flush()  # Ensure data is written to disk\n",
    "        \n",
    "        # Get the file path (just for demonstration)\n",
    "        print(f\"Using temp file: {temp_file.name}\")\n",
    "        \n",
    "        # Process the temp file (example: read it back)\n",
    "        temp_file.seek(0)\n",
    "        processed = temp_file.read().upper()\n",
    "        \n",
    "        # File is automatically deleted when context exits\n",
    "    return processed\n",
    "\n",
    "# Usage:\n",
    "# result = process_with_temp_file(\"Some temporary content\")\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other file I/O \n",
    "1. Processing excel, xml and sqlite files\n",
    "2. Reading and writing parqet files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "CSV files are essential because they provide a universal, lightweight, and easy way to store and exchange structured data across different platforms. Whether you're a programmer, data analyst, or business user, CSVs make data handling simple and efficient."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
